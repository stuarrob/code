{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETF Universe Collection & Update\n",
    "\n",
    "**Purpose:** Collect comprehensive universe of 2000+ ETFs from multiple sources\n",
    "\n",
    "**Run Frequency:** \n",
    "- Monthly for new ETF additions\n",
    "- Weekly for price data updates\n",
    "\n",
    "**Sources:**\n",
    "1. ETF Database (etfdb.com) - comprehensive ETF list\n",
    "2. Nasdaq listings - official exchange data\n",
    "3. Comprehensive seed list - curated by category\n",
    "\n",
    "**Process:**\n",
    "1. Scrape ETF tickers from multiple sources\n",
    "2. Merge and deduplicate\n",
    "3. Filter (remove leveraged, low AUM, etc.)\n",
    "4. Download price data in parallel (20 threads)\n",
    "5. Validate data quality\n",
    "6. Generate statistics and reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dynamic project root detection\n",
    "PROJECT_ROOT = Path().resolve()\n",
    "while not (PROJECT_ROOT / 'requirements.txt').exists() and PROJECT_ROOT != PROJECT_ROOT.parent:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Notebook executed: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "from src.data_collection.etf_universe_builder import ComprehensiveETFScraper, ParallelETFDownloader\n",
    "\n",
    "# Define paths\n",
    "DATA_DIR = PROJECT_ROOT / 'data' / 'raw'\n",
    "PRICES_DIR = DATA_DIR / 'prices'\n",
    "UNIVERSE_FILE = DATA_DIR / 'etf_universe.csv'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "\n",
    "# Create directories\n",
    "PRICES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"âœ“ Modules imported successfully\")\n",
    "print(f\"âœ“ Data directory: {DATA_DIR}\")\n",
    "print(f\"âœ“ Prices directory: {PRICES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scrape ETF Universe from Multiple Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scraper\n",
    "scraper = ComprehensiveETFScraper()\n",
    "\n",
    "print(\"Collecting ETFs from multiple sources...\\n\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 1: ETF Database (primary comprehensive source)\n",
    "print(\"\\n[1/3] Scraping ETF Database (etfdb.com)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "etfdb_etfs = scraper.scrape_etfdb_all_etfs(max_pages=50)\n",
    "\n",
    "print(f\"\\nâœ“ ETF Database: {len(etfdb_etfs)} ETFs collected\")\n",
    "print(f\"  Sample: {list(etfdb_etfs['ticker'].head(10))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 2: Nasdaq listings\n",
    "print(\"\\n[2/3] Scraping Nasdaq ETF listings...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "nasdaq_etfs = scraper.scrape_nasdaq_listings()\n",
    "\n",
    "print(f\"\\nâœ“ Nasdaq: {len(nasdaq_etfs)} ETFs collected\")\n",
    "if len(nasdaq_etfs) > 0:\n",
    "    print(f\"  Sample: {list(nasdaq_etfs['ticker'].head(10))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 3: Comprehensive seed list (backup/supplement)\n",
    "print(\"\\n[3/3] Loading comprehensive seed list...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "seed_etfs = scraper._get_comprehensive_seed_list()\n",
    "\n",
    "print(f\"\\nâœ“ Seed List: {len(seed_etfs)} ETFs\")\n",
    "print(f\"  Categories: {seed_etfs['category'].nunique()}\")\n",
    "print(f\"  Sample categories: {list(seed_etfs['category'].unique()[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Merge and Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMerging and deduplicating sources...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Merge all sources\n",
    "all_sources = [etfdb_etfs, nasdaq_etfs, seed_etfs]\n",
    "merged_universe = scraper.merge_and_deduplicate(all_sources)\n",
    "\n",
    "print(f\"\\nâœ“ Total unique ETFs after merge: {len(merged_universe)}\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  - ETF Database: {len(etfdb_etfs)}\")\n",
    "print(f\"  - Nasdaq: {len(nasdaq_etfs)}\")\n",
    "print(f\"  - Seed List: {len(seed_etfs)}\")\n",
    "print(f\"  - Unique merged: {len(merged_universe)}\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nPreview of merged universe:\")\n",
    "display(merged_universe.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Filter Universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFiltering ETF universe...\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nFilters applied:\")\n",
    "print(\"  - Remove leveraged/inverse ETFs\")\n",
    "print(\"  - Remove low AUM (<$10M)\")\n",
    "print(\"  - Clean ticker symbols\")\n",
    "print()\n",
    "\n",
    "filtered_universe = scraper.filter_universe(\n",
    "    merged_universe,\n",
    "    min_aum=10e6,  # $10M minimum\n",
    "    remove_leveraged=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Filtered universe: {len(filtered_universe)} ETFs\")\n",
    "print(f\"  Removed: {len(merged_universe) - len(filtered_universe)} ETFs\")\n",
    "\n",
    "# Show category distribution if available\n",
    "if 'category' in filtered_universe.columns:\n",
    "    print(f\"\\nCategory distribution:\")\n",
    "    category_counts = filtered_universe['category'].value_counts().head(15)\n",
    "    for cat, count in category_counts.items():\n",
    "        print(f\"  {cat:30s}: {count:4d} ETFs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Parallel Price Data Download\n",
    "\n",
    "**This will take 30-90 minutes depending on number of ETFs and network speed**\n",
    "\n",
    "Progress will be shown every 50 ETFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parallel downloader\n",
    "downloader = ParallelETFDownloader(\n",
    "    output_dir=PRICES_DIR,\n",
    "    min_years=2.0,  # Require minimum 2 years of data\n",
    "    max_workers=20,  # 20 parallel downloads\n",
    "    max_retries=3\n",
    ")\n",
    "\n",
    "print(\"Parallel Downloader Configuration:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  Output directory: {PRICES_DIR}\")\n",
    "print(f\"  Minimum data: {downloader.min_years} years ({downloader.min_days} days)\")\n",
    "print(f\"  Parallel workers: {downloader.max_workers}\")\n",
    "print(f\"  Max retries per ETF: {downloader.max_retries}\")\n",
    "print(f\"\\n  ETFs to download: {len(filtered_universe)}\")\n",
    "print(f\"  Estimated time: {len(filtered_universe) / (downloader.max_workers * 2):.0f}-{len(filtered_universe) / downloader.max_workers:.0f} minutes\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download price data\n",
    "print(\"\\nStarting parallel download...\")\n",
    "print(\"=\" * 80)\n",
    "print(\"(Progress updates every 50 ETFs)\\n\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Get list of tickers\n",
    "tickers = filtered_universe['ticker'].tolist()\n",
    "\n",
    "# Download\n",
    "download_results = downloader.download_batch(tickers)\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds() / 60\n",
    "\n",
    "print(f\"\\nâœ“ Download completed in {duration:.1f} minutes\")\n",
    "print(f\"  Rate: {len(tickers) / duration:.1f} ETFs/minute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download statistics\n",
    "print(\"\\nDownload Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total = len(download_results)\n",
    "successful = download_results['success'].sum()\n",
    "failed = total - successful\n",
    "success_rate = (successful / total) * 100\n",
    "\n",
    "print(f\"  Total ETFs: {total}\")\n",
    "print(f\"  Successful: {successful} ({success_rate:.1f}%)\")\n",
    "print(f\"  Failed: {failed} ({100-success_rate:.1f}%)\")\n",
    "\n",
    "# Show failure reasons\n",
    "if failed > 0:\n",
    "    print(f\"\\nTop failure reasons:\")\n",
    "    failed_results = download_results[~download_results['success']]\n",
    "    \n",
    "    # Categorize failures\n",
    "    failure_categories = {}\n",
    "    for msg in failed_results['message']:\n",
    "        if 'No data' in msg:\n",
    "            failure_categories['No data returned'] = failure_categories.get('No data returned', 0) + 1\n",
    "        elif 'Insufficient' in msg:\n",
    "            failure_categories['Insufficient history'] = failure_categories.get('Insufficient history', 0) + 1\n",
    "        elif 'missing' in msg.lower():\n",
    "            failure_categories['Too much missing data'] = failure_categories.get('Too much missing data', 0) + 1\n",
    "        else:\n",
    "            failure_categories['Other errors'] = failure_categories.get('Other errors', 0) + 1\n",
    "    \n",
    "    for reason, count in sorted(failure_categories.items(), key=lambda x: -x[1]):\n",
    "        print(f\"  - {reason}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update universe with download results\n",
    "print(\"\\nUpdating universe with download results...\")\n",
    "\n",
    "# Merge results\n",
    "final_universe = filtered_universe.merge(\n",
    "    download_results[['ticker', 'success', 'message']], \n",
    "    on='ticker', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add collection date\n",
    "final_universe['data_collection_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "# Save complete universe\n",
    "final_universe.to_csv(UNIVERSE_FILE, index=False)\n",
    "print(f\"âœ“ Saved universe to: {UNIVERSE_FILE}\")\n",
    "\n",
    "# Save download results\n",
    "results_file = RESULTS_DIR / f\"etf_download_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "download_results.to_csv(results_file, index=False)\n",
    "print(f\"âœ“ Saved download results to: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate downloaded data\n",
    "print(\"\\nValidating price data quality...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get successfully downloaded ETFs\n",
    "successful_tickers = download_results[download_results['success']]['ticker'].tolist()\n",
    "\n",
    "# Analyze a sample of files\n",
    "validation_stats = []\n",
    "\n",
    "for ticker in successful_tickers[:100]:  # Sample first 100\n",
    "    file_path = PRICES_DIR / f\"{ticker}.csv\"\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, index_col=0, parse_dates=True)\n",
    "        \n",
    "        validation_stats.append({\n",
    "            'ticker': ticker,\n",
    "            'num_days': len(df),\n",
    "            'start_date': df.index.min(),\n",
    "            'end_date': df.index.max(),\n",
    "            'missing_close_pct': df['Close'].isna().sum() / len(df) * 100,\n",
    "            'missing_volume_pct': df['Volume'].isna().sum() / len(df) * 100\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "validation_df = pd.DataFrame(validation_stats)\n",
    "\n",
    "if len(validation_df) > 0:\n",
    "    print(f\"\\nValidation sample: {len(validation_df)} ETFs\")\n",
    "    print(f\"\\nData coverage:\")\n",
    "    print(f\"  Average days of data: {validation_df['num_days'].mean():.0f}\")\n",
    "    print(f\"  Min days: {validation_df['num_days'].min():.0f}\")\n",
    "    print(f\"  Max days: {validation_df['num_days'].max():.0f}\")\n",
    "    print(f\"  Average missing Close: {validation_df['missing_close_pct'].mean():.2f}%\")\n",
    "    print(f\"  Average missing Volume: {validation_df['missing_volume_pct'].mean():.2f}%\")\n",
    "    \n",
    "    print(f\"\\nDate range:\")\n",
    "    print(f\"  Earliest start: {validation_df['start_date'].min()}\")\n",
    "    print(f\"  Latest end: {validation_df['end_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Summary & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL ETF UNIVERSE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nCollection Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\nETF Counts:\")\n",
    "print(f\"  Total scraped: {len(merged_universe)}\")\n",
    "print(f\"  After filtering: {len(filtered_universe)}\")\n",
    "print(f\"  Successfully downloaded: {successful}\")\n",
    "print(f\"  Failed downloads: {failed}\")\n",
    "print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "print(f\"\\nData Quality (sample of {len(validation_df)} ETFs):\")\n",
    "if len(validation_df) > 0:\n",
    "    print(f\"  Average history: {validation_df['num_days'].mean():.0f} days ({validation_df['num_days'].mean()/365:.1f} years)\")\n",
    "    print(f\"  Data completeness: {100 - validation_df['missing_close_pct'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\nFiles saved:\")\n",
    "print(f\"  Universe: {UNIVERSE_FILE}\")\n",
    "print(f\"  Price data: {PRICES_DIR} ({successful} files)\")\n",
    "print(f\"  Results: {results_file}\")\n",
    "\n",
    "print(f\"\\nâœ“ ETF Universe Collection Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Success/Failure pie chart\n",
    "axes[0, 0].pie(\n",
    "    [successful, failed],\n",
    "    labels=['Successful', 'Failed'],\n",
    "    autopct='%1.1f%%',\n",
    "    colors=['#2ecc71', '#e74c3c'],\n",
    "    startangle=90\n",
    ")\n",
    "axes[0, 0].set_title(f'Download Results ({total} ETFs)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 2. Data history distribution\n",
    "if len(validation_df) > 0:\n",
    "    axes[0, 1].hist(validation_df['num_days'], bins=30, color='#3498db', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Days of Historical Data')\n",
    "    axes[0, 1].set_ylabel('Number of ETFs')\n",
    "    axes[0, 1].set_title('Data History Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].axvline(730, color='red', linestyle='--', label='2 years')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "# 3. Category distribution (if available)\n",
    "if 'category' in final_universe.columns:\n",
    "    successful_etfs = final_universe[final_universe['success'] == True]\n",
    "    if len(successful_etfs) > 0:\n",
    "        category_counts = successful_etfs['category'].value_counts().head(10)\n",
    "        axes[1, 0].barh(range(len(category_counts)), category_counts.values, color='#9b59b6')\n",
    "        axes[1, 0].set_yticks(range(len(category_counts)))\n",
    "        axes[1, 0].set_yticklabels(category_counts.index)\n",
    "        axes[1, 0].set_xlabel('Number of ETFs')\n",
    "        axes[1, 0].set_title('Top 10 Categories (Successful Downloads)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].invert_yaxis()\n",
    "\n",
    "# 4. Missing data percentage\n",
    "if len(validation_df) > 0:\n",
    "    axes[1, 1].hist(validation_df['missing_close_pct'], bins=20, color='#e67e22', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Missing Data (%)')\n",
    "    axes[1, 1].set_ylabel('Number of ETFs')\n",
    "    axes[1, 1].set_title('Data Completeness Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].axvline(5, color='red', linestyle='--', label='5% threshold')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / f\"etf_universe_summary_{datetime.now().strftime('%Y%m%d')}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ“ Visualization saved to: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display successful ETFs sample\n",
    "print(\"\\nSample of successfully downloaded ETFs:\")\n",
    "successful_sample = final_universe[final_universe['success'] == True][['ticker', 'name', 'category', 'aum']].head(20)\n",
    "display(successful_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Universe is ready!** You can now:\n",
    "\n",
    "1. âœ… **Proceed to Phase 3:** Portfolio Optimization with small sample (100 ETFs)\n",
    "2. âœ… **Use full universe:** All successfully downloaded ETFs available\n",
    "3. ðŸ”„ **Update monthly:** Re-run this notebook to add new ETFs\n",
    "4. ðŸ”„ **Update prices weekly:** Re-run Step 4 only to refresh price data\n",
    "\n",
    "**Files created:**\n",
    "- `data/raw/etf_universe.csv` - Complete universe with metadata\n",
    "- `data/raw/prices/*.csv` - Individual ETF price files\n",
    "- `results/etf_download_results_*.csv` - Download log\n",
    "- `results/etf_universe_summary_*.png` - Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
